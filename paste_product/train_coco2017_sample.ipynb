{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb_import import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "yolo_dir = Path.home()/'new_yolov5/yolov5'\n",
    "if yolo_dir.is_dir():\n",
    "    sys.path.append(str(yolo_dir))\n",
    "\n",
    "from utils.tools.general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilkron_cy\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mount: /mnt/disks/lightbox_china_train: /dev/sdb already mounted on /mnt/disks/lightbox_china_train.\n",
      "mount: /mnt/disks/lightbox_china_merged-val: /dev/sdc already mounted on /mnt/disks/lightbox_china_merged-val.\n",
      "mount: /mnt/disks/lightbox_hand_eccsd-copy: /dev/sdd already mounted on /mnt/disks/lightbox_hand_eccsd-copy.\n",
      "mount: /mnt/disks/coco-2017-2: /dev/sde already mounted on /mnt/disks/coco-2017-2.\n",
      "mount: /mnt/disks/pasted-val: /dev/sdf already mounted on /mnt/disks/pasted-val.\n",
      "NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n",
      "loop0     7:0    0  55.5M  1 loop /snap/core18/2246\n",
      "loop1     7:1    0  55.5M  1 loop /snap/core18/2253\n",
      "loop2     7:2    0  61.9M  1 loop /snap/core20/1169\n",
      "loop3     7:3    0  61.9M  1 loop /snap/core20/1242\n",
      "loop4     7:4    0 251.7M  1 loop /snap/google-cloud-sdk/206\n",
      "loop5     7:5    0 251.7M  1 loop /snap/google-cloud-sdk/207\n",
      "loop6     7:6    0  67.2M  1 loop /snap/lxd/21803\n",
      "loop7     7:7    0  67.2M  1 loop /snap/lxd/21835\n",
      "loop8     7:8    0  42.2M  1 loop /snap/snapd/14066\n",
      "loop9     7:9    0  32.5M  1 loop /snap/snapd/13640\n",
      "sda       8:0    0    20G  0 disk \n",
      "â”œâ”€sda1    8:1    0  19.9G  0 part /\n",
      "â”œâ”€sda14   8:14   0     4M  0 part \n",
      "â””â”€sda15   8:15   0   106M  0 part /boot/efi\n",
      "sdb       8:16   0    10G  0 disk /mnt/disks/lightbox_china_train\n",
      "sdc       8:32   0    10G  0 disk /mnt/disks/lightbox_china_merged-val\n",
      "sdd       8:48   0    10G  0 disk /mnt/disks/lightbox_hand_eccsd-copy\n",
      "sde       8:64   0    30G  0 disk /mnt/disks/coco-2017-2\n",
      "sdf       8:80   0    20G  0 disk /mnt/disks/pasted-val\n"
     ]
    }
   ],
   "source": [
    "'''mount disk for `lightbox_china_train` data'''\n",
    "\n",
    "\n",
    "\n",
    "def format_disk(device_name):\n",
    "\t!sudo mkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/{device_name}\n",
    "\n",
    "def mount_disk(device_name, mount_dir):\n",
    "\t!sudo mkdir -p /mnt/disks/{mount_dir}\n",
    "\t!sudo mount -o discard,defaults /dev/{device_name} /mnt/disks/{mount_dir}\n",
    "\t!sudo chmod a+w /mnt/disks/{mount_dir}\n",
    "\n",
    "mount_disk('sdb', 'lightbox_china_train')\n",
    "mount_disk('sdc', 'lightbox_china_merged-val')\n",
    "mount_disk('sdd', 'lightbox_hand_eccsd-copy')\n",
    "mount_disk('sde', 'coco-2017-2')\n",
    "mount_disk('sdf', 'pasted-val')\n",
    "!sudo lsblk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create filelist txt for val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yaml_dir = yolo_dir/'data'/'test_merge_training'\n",
    "# yaml_dir = yolo_dir/'data'/'fake_coco128'\n",
    "yaml_dir = yolo_dir/'data'/'sampled_coco2017'\n",
    "yaml_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dpath = (yolo_dir/'datasets'/'coco128_no_label')\n",
    "# train_dpath = yolo_dir/'utils/paster/examples/fake_coco_128'\n",
    "train_dpath = Path('/mnt/disks/coco-2017-2/coco2017/train2017')\n",
    "train_dpath.lls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9acbac51a0b4bf4bfcc788618e8fe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7cee04b2004a749a8db2468bc3a31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# train_img_df = create_path_df((train_dpath/'images'/'train2017').ls())\n",
    "# train_lbl_df = create_path_df((train_dpath/'labels'/'train2017').ls())\n",
    "train_img_df = create_path_df((train_dpath/'images').ls())\n",
    "train_lbl_df = create_path_df((train_dpath/'labels').ls())\n",
    "\n",
    "train_img_df = train_img_df[train_img_df['is_file']]\n",
    "train_lbl_df = train_lbl_df[train_lbl_df['is_file']]\n",
    "train_img_and_lbl_df, columns_to_show = join_img_and_lbl_df(train_img_df, train_lbl_df)\n",
    "\n",
    "# train_img_and_lbl_df.head()\n",
    "sampled_train_df = train_img_and_lbl_df.sample(120)\n",
    "train_img_paths = sampled_train_df['file_path-img'].values\n",
    "fu.f_writelines(train_img_paths, yaml_dir/'train_sampled_images.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "2         /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "3         /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "4         /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "6         /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "                                ...                        \n",
       "101262    /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "101263    /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "101265    /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "101266    /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "101267    /mnt/disks/coco-2017-2/coco2017/train2017/imag...\n",
       "Length: 82047, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_imgs = pd.Series((train_dpath/'images').ls())\n",
    "# found_img = train_img_and_lbl_df['file_path-img']\n",
    "# all_imgs[~all_imgs.isin(found_img)]\n",
    "\n",
    "# others = (train_dpath.parent/'train2017_others')\n",
    "# all_imgs[~all_imgs.isin(found_img)].apply(lambda x: Path(x).rename( others/Path(x).name))\n",
    "# train_img_and_lbl_df['parent-img'][~train_img_and_lbl_df['parent-img'].isin((train_dpath/'images').ls())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87f339a0b3544a98577d11de3e8ccda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3170781ea42a190b1478f16c03823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35650/2523803976.py:9: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  val_lbl_df = val_lbl_df[val_img_df['is_file']]\n"
     ]
    }
   ],
   "source": [
    "# val_dpath = Path('/mnt/disks/lightbox_china_merged-val/data')\n",
    "# val_dpath = train_dpath\n",
    "val_dpath = Path('/mnt/disks/coco-2017-2/coco2017/test2017')\n",
    "\n",
    "val_img_df = create_path_df((val_dpath/'images').ls())\n",
    "val_lbl_df = create_path_df((val_dpath/'labels').ls())\n",
    "\n",
    "val_img_df = val_img_df[val_img_df['is_file']]\n",
    "val_lbl_df = val_lbl_df[val_img_df['is_file']]\n",
    "val_img_and_lbl_df, columns_to_show = join_img_and_lbl_df(val_img_df, val_lbl_df)\n",
    "\n",
    "sampled_val_df = val_img_and_lbl_df.sample(40)\n",
    "val_img_paths = sampled_val_df['file_path-img'].values\n",
    "fu.f_writelines(val_img_paths, yaml_dir/'val_sampled_images.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/train_sampled_images.txt',\n",
       " '/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/val_sampled_images.txt')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt = yaml_dir/'train_sampled_images.txt'\n",
    "val_txt = yaml_dir/'val_sampled_images.txt'\n",
    "data_yaml = yaml_dir/'lightbox_test_data.yaml'\n",
    "\n",
    "str(train_txt), str(val_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''remove labels cache'''\n",
    "\n",
    "for lbl_cache in yaml_dir.ls('*.cache'):\n",
    "    print(lbl_cache)\n",
    "    lbl_cache.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {data_yaml}\n",
    "\n",
    "# # Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\n",
    "# path: /home/cheeyung/yolov5/datasets/coco128  # dataset root dir\n",
    "# train: images/train2017  # train images (relative to 'path') 128 images\n",
    "# val: /home/cheeyung/yolov5/data/test_training/val_sampled_images.txt  # val images (relative to 'path') 128 images\n",
    "\n",
    "# # Classes\n",
    "# nc: 80  # number of classes\n",
    "# names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "#         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "#         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "#         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "#         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "#         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "#         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "#         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "#         'hair drier', 'toothbrush']  # class names\n",
    "\n",
    "\n",
    "# # Download script/URL (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {data_yaml}\n",
    "\n",
    "\n",
    "# # path: /home/cheeyung/yolov5/datasets/coco128  # dataset root dir\n",
    "# train: /home/cheeyung/yolov5/data/test_training/train_sampled_images.txt\n",
    "# val: /home/cheeyung/yolov5/data/test_training/val_sampled_images.txt\n",
    "\n",
    "# # # Classes\n",
    "# nc: 80  # number of classes\n",
    "# names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "#         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "#         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "#         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "#         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "#         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "#         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "#         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "#         'hair drier', 'toothbrush']  # class names\n",
    "\n",
    "\n",
    "# # # Download script/URL (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/train_sampled_images.txt',\n",
       " '/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/val_sampled_images.txt',\n",
       " True,\n",
       " True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt = yaml_dir/'train_sampled_images.txt'\n",
    "val_txt = yaml_dir/'val_sampled_images.txt'\n",
    "\n",
    "str(train_txt), str(val_txt), train_txt.is_file(), val_txt.is_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/lightbox_test_data.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {data_yaml}\n",
    "\n",
    "train: /home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/train_sampled_images.txt\n",
    "val:  /home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/val_sampled_images.txt\n",
    "\n",
    "\n",
    "nc: 14\n",
    "\n",
    "names:  ['wangzai_milk',\n",
    " 'wangwang_coco_vanilla',\n",
    " 'wangwang_coco_orange',\n",
    " 'vitamilk',\n",
    " 'lixing_yogurt_strawberry',\n",
    " 'kangshifu_green_tea',\n",
    " 'tongyi_milktea',\n",
    " 'kangshifu_beefNoodle',\n",
    " 'wanglaoji',\n",
    " 'sprite_200',\n",
    " 'pocky_coco',\n",
    " 'lixing_yogurt_kiwi',\n",
    " 'jiaduobao_550ml',\n",
    " 'kangshifu_peach_drink']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyp yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_custom_yaml = yaml_dir/'hyp_custom.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/hyp_custom.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {hyp_custom_yaml}\n",
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.05  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.05  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.5  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0  # image flip up-down (probability)\n",
    "fliplr: 0.3  # image flip left-right (probability)\n",
    "mosaic: 0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "copy_paste: 0.0  # segment copy-paste (probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/hyp_custom.yaml\n"
     ]
    }
   ],
   "source": [
    "# %%writefile {hyp_custom_yaml}\n",
    "# # Hyperparameter Evolution Results\n",
    "# # Generations: 50\n",
    "# # Metrics:      0.519     0.968     0.938     0.657    0.0108   0.00224   0.00387\n",
    "\n",
    "# lr0: 0.00428\n",
    "# lrf: 0.106\n",
    "# momentum: 0.878\n",
    "# weight_decay: 0.00036\n",
    "# warmup_epochs: 1.08\n",
    "# warmup_momentum: 0.547\n",
    "# warmup_bias_lr: 0.0471\n",
    "# box: 0.0264\n",
    "# cls: 0.263\n",
    "# cls_pw: 1.01\n",
    "# obj: 0.301\n",
    "# obj_pw: 1.25\n",
    "# iou_t: 0.3\n",
    "# anchor_t: 3.29\n",
    "# # anchors: 3.53\n",
    "# fl_gamma: 0.0\n",
    "# hsv_h: 0.01\n",
    "# hsv_s: 0.2\n",
    "# hsv_v: 0.2\n",
    "# degrees: 0.876\n",
    "# translate: 0.05\n",
    "# scale: 0.05\n",
    "# shear: 0.05\n",
    "# perspective: 0.0\n",
    "# flipud: 0.3\n",
    "# fliplr: 0.3\n",
    "# mosaic: 0\n",
    "# mixup: 0.0\n",
    "# copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_test_train(img_size):\n",
    "    '''continuous intergration testing '''\n",
    "\n",
    "    !python train.py --project {PROJECT} \\\n",
    "        --name {save_name} \\\n",
    "        --hyp {hyp_custom_yaml} \\\n",
    "        --data {data_yaml} \\\n",
    "        --cfg {cfg_yaml} \\\n",
    "        --epochs {epochs} \\\n",
    "        --batch-size {batch_size} \\\n",
    "        --img-size {img_size} \\\n",
    "        # --save_period & \\\n",
    "        --noautoanchor\n",
    "\n",
    "def training_cli():\n",
    "    !python train.py --project {PROJECT} \\\n",
    "        --name {save_name} \\\n",
    "        --hyp {hyp_custom_yaml} \\\n",
    "        --data {data_yaml} \\\n",
    "        --cfg {cfg_yaml} \\\n",
    "        --epochs {epochs} \\\n",
    "        --batch-size {batch_size} \\\n",
    "        --img-size {img_size} \\\n",
    "        --save_period 2 \\\n",
    "        --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cheeyung/new_yolov5/yolov5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/hyp_custom.yaml'),\n",
       " PosixPath('/home/cheeyung/new_yolov5/yolov5/models/yolov5m.yaml'),\n",
       " PosixPath('/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/lightbox_test_data.yaml'))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_custom_yaml = hyp_custom_yaml.resolve()\n",
    "cfg_yaml = (yolo_dir/'models/yolov5m.yaml').resolve()\n",
    "data_yaml = data_yaml.resolve()\n",
    "\n",
    "%cd {yolo_dir}\n",
    "\n",
    "(hyp_custom_yaml, cfg_yaml, data_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilkron_cy\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5n.pt, cfg=yolov5n.yaml, data=/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/lightbox_test_data.yaml, hyp=/home/cheeyung/new_yolov5/yolov5/data/sampled_coco2017/hyp_custom.yaml, epochs=1, batch_size=16, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mâš ï¸ YOLOv5 is out of date by 114 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 ðŸš€ v6.0-2-g3a3aca8 torch 1.7.0+cu110 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.05, hsv_v=0.05, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.3, mosaic=0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcelestial-brook-44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/silkron_cy/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/silkron_cy/YOLOv5/runs/1pi541y7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/cheeyung/new_yolov5/yolov5/wandb/run-20211201_075641-1pi541y7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 53092... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.00011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.0102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.10051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.073\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.01436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.11494\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.08346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.02428\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.09937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 44 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcelestial-brook-44\u001b[0m: \u001b[34mhttps://wandb.ai/silkron_cy/YOLOv5/runs/1pi541y7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211201_075641-1pi541y7/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "Results saved to \u001b[1mruns/train/exp9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 320 --batch 16 --weights yolov5n.pt --cfg yolov5n.yaml --epochs 1 --device 'cpu' --data {data_yaml} --hyp {hyp_custom_yaml}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilkron_cy\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "usage: train.py [-h] [--weights WEIGHTS] [--cfg CFG] [--data DATA] [--hyp HYP]\n",
      "  --hyp HYP             hyperparameters path\n",
      "  --evolve [EVOLVE]     evolve hyperparameters for x generations\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help | grep hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msilkron_cy\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=/home/cheeyung/new_yolov5/yolov5/models/yolov5m.yaml, data=/home/cheeyung/new_yolov5/yolov5/data/fake_coco128/lightbox_test_data.yaml, hyp=/home/cheeyung/new_yolov5/yolov5/data/fake_coco128/hyp_custom.yaml, epochs=1, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=fake_coco128--debug, name=v6_master--hsv, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mâš ï¸ YOLOv5 is out of date by 114 commits. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 ðŸš€ v6.0-2-g3a3aca8 torch 1.7.0+cu110 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.00428, lrf=0.106, momentum=0.878, weight_decay=0.00036, warmup_epochs=1.08, warmup_momentum=0.547, warmup_bias_lr=0.0471, box=0.0264, cls=0.263, cls_pw=1.01, obj=0.301, obj_pw=1.25, iou_t=0.3, anchor_t=3.29, fl_gamma=0.0, hsv_h=0.01, hsv_s=0.2, hsv_v=0.2, degrees=0.876, translate=0.05, scale=0.05, shear=0.05, perspective=0.0, flipud=0.3, fliplr=0.3, mosaic=0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir fake_coco128--debug', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv6_master--hsv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/silkron_cy/fake_coco128--debug\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/silkron_cy/fake_coco128--debug/runs/1lysevwk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/cheeyung/new_yolov5/yolov5/wandb/run-20211201_025854-1lysevwk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2254... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.03506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.0239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.00795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.0318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.02214\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.00716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00012\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.04586\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 13 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mv6_master--hsv\u001b[0m: \u001b[34mhttps://wandb.ai/silkron_cy/fake_coco128--debug/runs/1lysevwk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211201_025854-1lysevwk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "Results saved to \u001b[1mfake_coco128--debug/v6_master--hsv2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''jump'''\n",
    "# img_size = 1024\n",
    "\n",
    "img_size = 640\n",
    "# batch_size = 32 if img_size < 1024 else 16\n",
    "batch_size = 4\n",
    "print(f\"batch_size:  {batch_size}\")\n",
    "\n",
    "\n",
    "PROJECT = 'fake_coco128--debug'\n",
    "# PROJECT = 'lightbox_val_merged--debug'\n",
    "save_name = 'v6_master--hsv'\n",
    "# save_name = f'labeled_coco128_sample120--{img_size}-'\n",
    "epochs = 1\n",
    "\n",
    "ci_test_train(img_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_img(project, save_name):\n",
    "    result_dirs = (yolo_dir/project).ls(save_name+'*')\n",
    "    \n",
    "    if not any(p.is_dir() for p in result_dirs):\n",
    "        print('Result Directory not found')\n",
    "    else:\n",
    "        latest_run = sorted(result_dirs)[-1]\n",
    "        img_paths = latest_run.ls('train_batch*.jpg')\n",
    "\n",
    "        if len(img_paths):\n",
    "            print('train img found')\n",
    "            [ia.imshow(imageio.imread(im)) for im in img_paths]\n",
    "    \n",
    "\n",
    "plot_train_img(PROJECT, save_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse arg for debug training in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from test_train import *\n",
    "\n",
    "\n",
    "# parse_arg_notetbook = (yolo_dir/'paste_product'/'parse_args.ipynb')\n",
    "# if parse_arg_notetbook.is_file():\n",
    "#     !jupyter nbconvert --to script {parse_arg_notetbook} \n",
    "# else:\n",
    "#     raise ValueError('Notebook not found')\n",
    "\n",
    "# from paste_product.parse_args import get_default_args, merge_args, parse_args, print_if_keys_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_args(train_options, file_path_args):\n",
    "    merged_train_args = merge_args(train_options, file_path_args)\n",
    "\n",
    "    print_if_keys_in_dict(train_options, merged_train_args)\n",
    "    print_if_keys_in_dict(file_path_args, merged_train_args)\n",
    "    print_if_keys_in_dict(get_default_args(), merged_train_args)\n",
    "\n",
    "    train_namespace = parse_args(train_options, file_path_args)\n",
    "\n",
    "    return train_namespace\n",
    "\n",
    "def parse_debug_opt(train_options, file_path_args):\n",
    "    opt = init_args(train_options, file_path_args)\n",
    "    opt.resume = False\n",
    "    return opt\n",
    "\n",
    "def training_with_opt_in_nb():\n",
    "\n",
    "    def debug_training():\n",
    "        opt = parse_debug_opt(train_options, file_path_args)\n",
    "        print(opt)\n",
    "        main(opt)\n",
    "\n",
    "\n",
    "\n",
    "    file_path_args = {\n",
    "        'hyp' : hyp_custom_yaml,\n",
    "        'data' : data_yaml,\n",
    "        'weights' : yolo_dir/'yolov5s.pt',\n",
    "        'cfg' : yolo_dir/ 'models/yolov5s.yaml'\n",
    "    }\n",
    "\n",
    "\n",
    "    train_options = {\n",
    "        'project' : 'lightbox_val_merged--debug',\n",
    "        # 'name' : 'solved_filter_front_img_in_dataset',\n",
    "        'name' : 'img_not_log_in_notebook_arg',\n",
    "\n",
    "        'epochs' : 1,\n",
    "        'batch_size' : 16,\n",
    "        'imgsz' : 160,\n",
    "        'nosave' : False,\n",
    "        'upload_dataset': False,\n",
    "        'save_period': -1,\n",
    "    }\n",
    "\n",
    "    # init_args(train_options, file_path_args)\n",
    "    debug_training()\n",
    "\n",
    "# training_with_opt_in_nb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json dump namespace to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56268/3967507451.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdump_nampespace_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_debug_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_options' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# import json\n",
    "\n",
    "# def dump_nampespace_to_file(namespace_opt, save_file='opt_dict.txt'):\n",
    "#     opt_dict = vars(namespace_opt)\n",
    "#     for k,v in opt_dict.items():\n",
    "#         if isinstance(v, Path):\n",
    "#             opt_dict[k] = str(v)\n",
    "#     with open(save_file, 'w') as f:\n",
    "#         json.dump(opt_dict, f)\n",
    "\n",
    "\n",
    "# dump_nampespace_to_file(parse_debug_opt(train_options, file_path_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndump_nampespace_to_file -> train_cli_opt\\ninit_args(train_options, file_path_args) -> notebook_opt\\n\\ncompare_with_df(train_cli_opt, vars(notebook_opt))\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dump_nampespace_to_file -> train_cli_opt\n",
    "init_args(train_options, file_path_args) -> notebook_opt\n",
    "\n",
    "compare_with_df(train_cli_opt, vars(notebook_opt))\n",
    "'''\n",
    "\n",
    "# test_db_opt = init_args(train_options, file_path_args)\n",
    "# pprint(vars(test_db_opt))\n",
    "\n",
    "# col = 'hyp'\n",
    "# print(f\"opt_df['ori'][{col}]:  {opt_df['ori'][col]}\")\n",
    "# f\"opt_df['mine'][{col}]:  {opt_df['mine'][col]}\"\n",
    "\n",
    "# with open('opt_dict.txt', 'r') as f:\n",
    "#     json_ori_train_opt = json.load(f)\n",
    "\n",
    "# opt_df = pd.DataFrame([json_ori_train_opt, vars(test_db_opt)], index=['ori', 'mine']).T\n",
    "# has_same_opt = (opt_df['ori'] == opt_df['mine'])\n",
    "# opt_df[~has_same_opt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug test\n",
    "(nid to remove label's .cache file)\n",
    "\n",
    "- real labeled coco128\n",
    "    - by dir (all img) = pass\n",
    "    - by txt (sample 10) = train not upload, val img pass\n",
    "    - by txt (sample 120) \n",
    "        - im_size 320 = pass\n",
    "        - im_size 160 = pass\n",
    "\n",
    "- fake labeled coco128 (dummy label with only 13 class)\n",
    "    - by txt (sample 120) \n",
    "        - im_size 160 = pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04ddf35340c1ce64f127aae7421c8cf99ef6dbb93c27b1d0248a5253fd4efee9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('yolov5': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
